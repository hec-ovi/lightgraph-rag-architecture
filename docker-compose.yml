services:
  ollama:
    build: ./ollama
    image: lightrag-ollama:latest
    container_name: lightrag-ollama
    privileged: true
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ${OLLAMA_MODELS_DIR}:/ollama-models
    environment:
      - OLLAMA_MODEL=${OLLAMA_MODEL:-gpt-oss:20b}
      - OLLAMA_EMBED_MODEL=${OLLAMA_EMBED_MODEL:-bge-m3:latest}
      - OLLAMA_MODELS=/ollama-models
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_CONTEXT_LENGTH=${OLLAMA_CONTEXT_LENGTH:-32768}
      - OLLAMA_FLASH_ATTENTION=1
      - OLLAMA_KV_CACHE_TYPE=q8_0
      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:--1}
      - OLLAMA_WARMUP=${OLLAMA_WARMUP:-true}
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_LOADED_MODELS=2
      - HSA_OVERRIDE_GFX_VERSION=11.5.1
      - HIP_VISIBLE_DEVICES=0
      - AMD_SERIALIZE_KERNEL=1
    ipc: host
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      start_period: 120s
      retries: 5

  backend:
    build: ./backend
    image: lightrag-backend:latest
    container_name: lightrag-backend
    restart: unless-stopped
    ports:
      - "${BACKEND_PORT:-8000}:8000"
    volumes:
      - ./backend/data:/app/data
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-gpt-oss:20b}
      - OLLAMA_EMBED_MODEL=${OLLAMA_EMBED_MODEL:-bge-m3:latest}
      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:--1}
      - OLLAMA_REQUEST_TIMEOUT_SECONDS=${OLLAMA_REQUEST_TIMEOUT_SECONDS:-900}
      - OLLAMA_EMBED_TIMEOUT_SECONDS=${OLLAMA_EMBED_TIMEOUT_SECONDS:-300}
      - OLLAMA_HEALTH_TIMEOUT_SECONDS=${OLLAMA_HEALTH_TIMEOUT_SECONDS:-5}
      - LLM_TIMEOUT=${LLM_TIMEOUT:-900}
      - EMBEDDING_TIMEOUT=${EMBEDDING_TIMEOUT:-300}
      - LIGHTRAG_CONTEXT_WINDOW=${LIGHTRAG_CONTEXT_WINDOW:-32768}
      - LIGHTRAG_EMBEDDING_DIM=${LIGHTRAG_EMBEDDING_DIM:-1024}
      - LIGHTRAG_EMBEDDING_MAX_TOKENS=${LIGHTRAG_EMBEDDING_MAX_TOKENS:-8192}
    depends_on:
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      start_period: 30s
      retries: 3

  frontend:
    build: ./frontend
    image: lightrag-frontend:latest
    container_name: lightrag-frontend
    restart: unless-stopped
    ports:
      - "${FRONTEND_PORT:-5173}:80"
    depends_on:
      backend:
        condition: service_healthy
